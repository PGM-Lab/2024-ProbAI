{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PGM-Lab/2023-probai-private/blob/main/python/Day2-AfterLunch/notebooks/BayesianNeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmpXVYmZzO5v"
   },
   "source": [
    "# Bayesian Neural Networks\n",
    "\n",
    "Neural networks are powerful approximators. However, standard approaches for learning this approximators does not take into account the inherent uncertainty we may have when fitting a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bip2DylCzO5y"
   },
   "outputs": [],
   "source": [
    "%pip install -q numpy pyro-ppl torch matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1TMLxiAzO5z"
   },
   "source": [
    "## Data\n",
    "\n",
    "We use some fake data. As neural nets of even one hidden layer\n",
    "can be universal function approximators, we can see if we can\n",
    "train a simple neural network to fit a noisy sinusoidal data, like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "0hn6YKlRzO50",
    "outputId": "8b954ac5-a8f1-402c-bd95-bb336fdbb48a"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data\n",
    "def gold_standard(x):\n",
    "    return .5 * x + 7. * np.sin(0.75 * x)\n",
    "\n",
    "NSAMPLE = 600\n",
    "x_train = np.concatenate(\n",
    "    (np.random.uniform(-10, 0, ((2 * NSAMPLE)//3, 1)),np.random.uniform(5, 10, (NSAMPLE - (2 * NSAMPLE)//3, 1))),\n",
    "    axis=0)\n",
    "y_train = gold_standard(x_train) + np.random.normal(size=(NSAMPLE,1), scale=3.0)\n",
    "x_test = np.arange(-20, 15, 0.05).reshape(-1,1)\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# Plot it. Notice the gap in observations\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_train, y_train, marker='+', label='Training data')\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y = .5x + 7\\\\sin(.75x) + \\\\epsilon$,  \" + \n",
    "           \"$\\\\epsilon\\\\sim N (\\\\mu=0, \\\\sigma^2=9)$\")\n",
    "plt.xlim([-10, 15])\n",
    "plt.ylim([-20, 20])\n",
    "plt.grid(True, \"both\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjCrT0OdzO50"
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "We employ a simple feedforward network with `NHIDDEN` hidden units to try to fit the data.\n",
    "\n",
    "<center>\n",
    "<img src=\"./BNNs-plain.png\" alt=\"Drawing\" height=\"250\">\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqO6MazHzO51",
    "outputId": "972beecc-3090-4e75-f1cd-02eee83288c1"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, NHIDDEN):\n",
    "        # Simple random initialization close to zero\n",
    "        self.w = torch.nn.Parameter(torch.randn(1, NHIDDEN) * 0.05)\n",
    "        self.b = torch.nn.Parameter(torch.randn(1, NHIDDEN) * 0.05)\n",
    "        self.w_out = torch.nn.Parameter(torch.randn(NHIDDEN,1) * 0.05)\n",
    "        self.b_out = torch.nn.Parameter(torch.randn(1, 1) * 0.05)\n",
    "\n",
    "    def params(self):\n",
    "        return {\"b\":self.b, \"w\": self.w, \"b_out\":self.b_out, \"w_out\": self.w_out}\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        hidden_layer = torch.nn.Tanh()(torch.mm(x_data, self.w) + self.b)\n",
    "        return torch.matmul(hidden_layer, self.w_out) + self.b_out\n",
    "\n",
    "NHIDDEN = 250\n",
    "neuralNetwork = NeuralNetwork(NHIDDEN)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optim = torch.optim.Adam(neuralNetwork.params().values(), lr=0.01)\n",
    "num_iterations = 10000\n",
    "\n",
    "for j in range(num_iterations + 1):\n",
    "    # run the model forward on the data\n",
    "    y_pred = neuralNetwork.predict(x_train)\n",
    "    # calculate the mse loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    # initialize gradients to zero\n",
    "    optim.zero_grad()\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    # take a gradient step\n",
    "    optim.step()\n",
    "    if j % 1000 == 0:\n",
    "        print(\"[iteration %05d] loss: %12.2f\" % (j, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqM7LuIIzO52"
   },
   "source": [
    "### Results:\n",
    "We see that the neural network can fit this sinusoidal data quite well, as expected. However, notice the behavour around the gap in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 774
    },
    "id": "fGe0X9-yzO53",
    "outputId": "28110acb-f658-4b18-da04-134b44371390"
   },
   "outputs": [],
   "source": [
    "y_test = neuralNetwork.predict(x_test.clone()).detach()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_train, y_train, marker='+', label='Training data')\n",
    "plt.plot(x_test, y_test, 'r-', label='Prediction');\n",
    "plt.title('Standard Neural Network')\n",
    "plt.legend();\n",
    "plt.xticks(np.arange(-10, 16, 5))\n",
    "plt.yticks(np.arange(-20, 21, 5))\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.xlim([-10, 15])\n",
    "plt.ylim([-20, 20])\n",
    "plt.grid(True, \"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHyju0nwzO53"
   },
   "source": [
    "However this model is unable to capture the uncertainty in the model. For example, when making predictions about a single point (e.g. around $x=2.5$) we can see we do not account aobut the inherent noise there is in this predictions or the uncertainty about what the model shouyld actually be in this area. In next section, we will what happen when we introduce a Bayesian approach using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRjL7TU_zO54"
   },
   "source": [
    "## Bayesian Learning of Neural Networks\n",
    "\n",
    "[Bayesian modeling](https://mlg.eng.cam.ac.uk/zoubin/papers/NatureReprint15.pdf) offers a systematic framework for reasoning about model uncertainty. Instead of just learning point estimates, we're going to learn a distribution over models that are consistent with the observed data.\n",
    "\n",
    "In Bayesian learning, the weights of the network are **random variables**. The output of the nework is also a **random variable**. The random variable of the output *implicitly defines the loss function*. So, when making Bayesian learning we do **not** define *loss functions*, we do define *random variables*, and fit calculate their *posterior* distributions.\n",
    "\n",
    "<center>\n",
    "<img src=\"./BNNs-prob.png\" alt=\"Drawing\" height=\"250\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctfQpqBUzO55"
   },
   "outputs": [],
   "source": [
    "def model(NHIDDEN, x_train, y_train):\n",
    "    # The Bayesian NN has random variables representing what was torch.nn.Parameter-s in the standard neural net.\n",
    "    W = pyro.sample(\"W\", pyro.distributions.Normal(loc = torch.zeros([1, NHIDDEN]), scale=1.).to_event())\n",
    "    b = pyro.sample(\"b\", pyro.distributions.Normal(loc = torch.zeros([1, NHIDDEN]), scale=1.).to_event())\n",
    "    W_out = pyro.sample(\"W_out\", pyro.distributions.Normal(loc = torch.zeros([NHIDDEN,1]), scale=1.).to_event())\n",
    "    b_out = pyro.sample(\"b_out\", pyro.distributions.Normal(loc = torch.zeros([1,1]), scale=1.).to_event())\n",
    "\n",
    "    # The model definition includes what we had in the .predict - method above \n",
    "    hidden_layer = torch.nn.Tanh()(torch.mm(x_train, W) + b)\n",
    "    out =  torch.matmul(hidden_layer,W_out) + b_out\n",
    "\n",
    "    # We want the model (given a value for all random variables defining weights and biases) to model a random variable. \n",
    "    # We do that by defining it as a Gaussian with mean given by the model's output, and fixed standfard dev 1.\n",
    "    predictive_mean = pyro.deterministic(\"predictive_mean\", out)\n",
    "    with pyro.plate(\"data\", x_train.shape[0]):\n",
    "        y = pyro.sample(\"y\", pyro.distributions.Normal(loc=predictive_mean, scale=1.).to_event(1), obs=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning is done by [Pyro](https://pyro.ai). We get access to `SVI`, a class that does the variational inference for us using the `guide`, which defines the set of variational approximations $\\mathcal{Q}$. We will disregard the details of this for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dWIuU2aozO56",
    "outputId": "a62a4121-4cc6-45eb-9486-60658fbf8a8a"
   },
   "outputs": [],
   "source": [
    "# Setup the optimizer. Never mind the details here, it is internal to Pyro\n",
    "pyro.clear_param_store()\n",
    "guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "svi = pyro.infer.SVI(model=model, guide=guide, loss=pyro.infer.Trace_ELBO(), optim=pyro.optim.Adam({\"lr\": 0.01}), )\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_iterations + 1):\n",
    "    loss = svi.step(NHIDDEN, x_train, y_train)\n",
    "    if (epoch % 500) == 0:\n",
    "        print(\"[epoch %05d] ELBO: %12.2f\" % (epoch, -loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLFLCMXGzO56"
   },
   "source": [
    "### Benefit: Explicit representation of uncertainty\n",
    "As can be seen in the next figure, the output of our model is no longer deterministic. Instead, we now caputure our uncertainty and make it explicit. Notice how we capture both epistemic (\"model-uncertainty\") and aleatoric (\"data randomness\") uncertainty. Consider in particular what happens with the predictions where we did not have training-data (around $x=2.5$ and for $x>10$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "KCbJssxsnyms",
    "outputId": "5a241f9b-85f7-45e2-cd3a-3ec8418dd3e5"
   },
   "outputs": [],
   "source": [
    "# Run model to generate samples. Each sample means all weights are sampled \n",
    "# from their posterior distributions. \n",
    "predictive = pyro.infer.Predictive(model, guide=guide, num_samples=5000)\n",
    "svi_samples = predictive(NHIDDEN, x_test, None)\n",
    "\n",
    "# Calculate per-x sample-mean and sample-std to show uncertainty bounds\n",
    "y_preds = torch.squeeze(svi_samples['y']).detach().numpy()\n",
    "y_mean = np.mean(y_preds, axis=0)\n",
    "y_sigma = np.std(y_preds, axis=0)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.fill_between(x_test.ravel(), \n",
    "                 y_mean + 2 * y_sigma, y_mean - 2 * y_sigma, alpha=0.5, \n",
    "                 color='orange', \n",
    "                 label = \"Posterior predictive distribution\")\n",
    "\n",
    "# Calculate the predictive mean. Note that we get one per mean-line sample of model weights, \n",
    "# hence we are able to expose model uncertainty. \n",
    "y_predictive_mean = torch.squeeze(svi_samples['predictive_mean']).detach().numpy()\n",
    "for i in range(np.min([100, y_predictive_mean.shape[0]])):\n",
    "      plt.plot(x_test, y_predictive_mean[i], color='darkred', linewidth=1, alpha=.15, label=\"Sampled alternative means\" if not i else None)\n",
    "plt.scatter(x_train, y_train, marker='+', label = \"Training data\")\n",
    "\n",
    "# Make the plot look good\n",
    "plt.xticks(np.arange(-10, 16, 5))\n",
    "plt.yticks(np.arange(-20, 21, 5))\n",
    "plt.title('Bayesian Neural Network')\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.xlim([-10, 15])\n",
    "plt.ylim([-20, 20])\n",
    "plt.grid(True, \"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0wLeEULzO57"
   },
   "source": [
    "### <span style=\"color:red\">Optional</span> exercise:  Fix the variance\n",
    "\n",
    "If you want to dig more into this in your own time, then consider the variance of the variable $y$ in the above model. It was assumed fixed at $\\sigma=1$ (a pooor choice since $\\sigma=3$ was used when we sampled the data) when we said that\n",
    "``y = pyro.sample(\"y\", pyro.distributions.Normal(loc=predictive_mean, scale=1.).to_event(1), obs=y_train)``.\n",
    "\n",
    "There are several ways to improve on this (beyond \"cheating\" and hardcoding that $\\sigma=3$):\n",
    "\n",
    "1. Introduce a Pyro parameter to learn the variance. **Hint:** Initialize with a very small value.\n",
    "\n",
    "2. Introduce a Pryro random variable to represent the variance, and find its posterior. **Hint:** Initialize the guide's random variable with a very small value.\n",
    "\n",
    "3. Make the variance dependent of the input data by learning a model that outputs $\\log\\sigma$ as a function of $x$.\n",
    "\n",
    "\n",
    "Evaluate the output for all cases and compare with the previous results.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "BayesianNeuralNetworks.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
